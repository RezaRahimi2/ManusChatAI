[Skip to content](https://github.com/BerriAI/litellm#start-of-content)

You signed in with another tab or window. [Reload](https://github.com/BerriAI/litellm) to refresh your session.You signed out in another tab or window. [Reload](https://github.com/BerriAI/litellm) to refresh your session.You switched accounts on another tab or window. [Reload](https://github.com/BerriAI/litellm) to refresh your session.Dismiss alert

[BerriAI](https://github.com/BerriAI)/ **[litellm](https://github.com/BerriAI/litellm)** Public

- Sponsor







# Sponsor BerriAI/litellm



















##### External links









[https://buy.stripe.com/9AQ03Kd3P91o0Q8bIS](https://buy.stripe.com/9AQ03Kd3P91o0Q8bIS)









[Learn more about funding links in repositories](https://docs.github.com/repositories/managing-your-repositorys-settings-and-features/customizing-your-repository/displaying-a-sponsor-button-in-your-repository).




[Report abuse](https://github.com/contact/report-abuse?report=BerriAI%2Flitellm+%28Repository+Funding+Links%29)

- [Notifications](https://github.com/login?return_to=%2FBerriAI%2Flitellm) You must be signed in to change notification settings
- [Fork\\
2.6k](https://github.com/login?return_to=%2FBerriAI%2Flitellm)
- [Star\\
20.2k](https://github.com/login?return_to=%2FBerriAI%2Flitellm)


Python SDK, Proxy Server (LLM Gateway) to call 100+ LLM APIs in OpenAI format - \[Bedrock, Azure, OpenAI, VertexAI, Cohere, Anthropic, Sagemaker, HuggingFace, Replicate, Groq\]


[docs.litellm.ai/docs/](https://docs.litellm.ai/docs/ "https://docs.litellm.ai/docs/")

### License

[View license](https://github.com/BerriAI/litellm/blob/main/LICENSE)

[20.2k\\
stars](https://github.com/BerriAI/litellm/stargazers) [2.6k\\
forks](https://github.com/BerriAI/litellm/forks) [Branches](https://github.com/BerriAI/litellm/branches) [Tags](https://github.com/BerriAI/litellm/tags) [Activity](https://github.com/BerriAI/litellm/activity)

[Star](https://github.com/login?return_to=%2FBerriAI%2Flitellm)

[Notifications](https://github.com/login?return_to=%2FBerriAI%2Flitellm) You must be signed in to change notification settings

# BerriAI/litellm

main

[**2369** Branches](https://github.com/BerriAI/litellm/branches) [**873** Tags](https://github.com/BerriAI/litellm/tags)

[Go to Branches page](https://github.com/BerriAI/litellm/branches)[Go to Tags page](https://github.com/BerriAI/litellm/tags)

Go to file

Code

## Folders and files

| Name | Name | Last commit message | Last commit date |
| --- | --- | --- | --- |
| ## Latest commit<br>[![krrishdholakia](https://avatars.githubusercontent.com/u/17561003?v=4&size=40)](https://github.com/krrishdholakia)[krrishdholakia](https://github.com/BerriAI/litellm/commits?author=krrishdholakia)<br>[fix(router.py): support reusable credentials via passthrough router (](https://github.com/BerriAI/litellm/commit/c555c15ad7d65850b3348436d91b910afc1c1907) [#…](https://github.com/BerriAI/litellm/pull/9758)<br>Apr 4, 2025<br>[c555c15](https://github.com/BerriAI/litellm/commit/c555c15ad7d65850b3348436d91b910afc1c1907) · Apr 4, 2025<br>## History<br>[21,265 Commits](https://github.com/BerriAI/litellm/commits/main/) |
| [.circleci](https://github.com/BerriAI/litellm/tree/main/.circleci ".circleci") | [.circleci](https://github.com/BerriAI/litellm/tree/main/.circleci ".circleci") | [update circle ci requirements](https://github.com/BerriAI/litellm/commit/12b392357981c4bfd7c3d1e87642f6b18c641c54 "update circle ci requirements") | Apr 4, 2025 |
| [.devcontainer](https://github.com/BerriAI/litellm/tree/main/.devcontainer ".devcontainer") | [.devcontainer](https://github.com/BerriAI/litellm/tree/main/.devcontainer ".devcontainer") | [LiteLLM Minor Fixes and Improvements (08/06/2024) (](https://github.com/BerriAI/litellm/commit/72e961af3c6e12a7e55f9744c35209164222a936 "LiteLLM Minor Fixes and Improvements (08/06/2024)  (#5567)  * fix(utils.py): return citations for perplexity streaming  Fixes https://github.com/BerriAI/litellm/issues/5535  * fix(anthropic/chat.py): support fallbacks for anthropic streaming (#5542)  * fix(anthropic/chat.py): support fallbacks for anthropic streaming  Fixes https://github.com/BerriAI/litellm/issues/5512  * fix(anthropic/chat.py): use module level http client if none given (prevents early client closure)  * fix: fix linting errors  * fix(http_handler.py): fix raise_for_status error handling  * test: retry flaky test  * fix otel type  * fix(bedrock/embed): fix error raising  * test(test_openai_batches_and_files.py): skip azure batches test (for now) quota exceeded  * fix(test_router.py): skip azure batch route test (for now) - hit batch quota limits  ---------  Co-authored-by: Ishaan Jaff <ishaanjaffer0324@gmail.com>  * All `model_group_alias` should show up in `/models`, `/model/info` , `/model_group/info` (#5539)  * fix(router.py): support returning model_alias model names in `/v1/models`  * fix(proxy_server.py): support returning model alias'es on `/model/info`  * feat(router.py): support returning model group alias for `/model_group/info`  * fix(proxy_server.py): fix linting errors  * fix(proxy_server.py): fix linting errors  * build(model_prices_and_context_window.json): add amazon titan text premier pricing information  Closes https://github.com/BerriAI/litellm/issues/5560  * feat(litellm_logging.py): log standard logging response object for pass through endpoints. Allows bedrock /invoke agent calls to be correctly logged to langfuse + s3  * fix(success_handler.py): fix linting error  * fix(success_handler.py): fix linting errors  * fix(team_endpoints.py): Allows admin to update team member budgets  ---------  Co-authored-by: Ishaan Jaff <ishaanjaffer0324@gmail.com>") [#5567](https://github.com/BerriAI/litellm/pull/5567) [)](https://github.com/BerriAI/litellm/commit/72e961af3c6e12a7e55f9744c35209164222a936 "LiteLLM Minor Fixes and Improvements (08/06/2024)  (#5567)  * fix(utils.py): return citations for perplexity streaming  Fixes https://github.com/BerriAI/litellm/issues/5535  * fix(anthropic/chat.py): support fallbacks for anthropic streaming (#5542)  * fix(anthropic/chat.py): support fallbacks for anthropic streaming  Fixes https://github.com/BerriAI/litellm/issues/5512  * fix(anthropic/chat.py): use module level http client if none given (prevents early client closure)  * fix: fix linting errors  * fix(http_handler.py): fix raise_for_status error handling  * test: retry flaky test  * fix otel type  * fix(bedrock/embed): fix error raising  * test(test_openai_batches_and_files.py): skip azure batches test (for now) quota exceeded  * fix(test_router.py): skip azure batch route test (for now) - hit batch quota limits  ---------  Co-authored-by: Ishaan Jaff <ishaanjaffer0324@gmail.com>  * All `model_group_alias` should show up in `/models`, `/model/info` , `/model_group/info` (#5539)  * fix(router.py): support returning model_alias model names in `/v1/models`  * fix(proxy_server.py): support returning model alias'es on `/model/info`  * feat(router.py): support returning model group alias for `/model_group/info`  * fix(proxy_server.py): fix linting errors  * fix(proxy_server.py): fix linting errors  * build(model_prices_and_context_window.json): add amazon titan text premier pricing information  Closes https://github.com/BerriAI/litellm/issues/5560  * feat(litellm_logging.py): log standard logging response object for pass through endpoints. Allows bedrock /invoke agent calls to be correctly logged to langfuse + s3  * fix(success_handler.py): fix linting error  * fix(success_handler.py): fix linting errors  * fix(team_endpoints.py): Allows admin to update team member budgets  ---------  Co-authored-by: Ishaan Jaff <ishaanjaffer0324@gmail.com>") | Sep 6, 2024 |
| [.github](https://github.com/BerriAI/litellm/tree/main/.github ".github") | [.github](https://github.com/BerriAI/litellm/tree/main/.github ".github") | [ci(test-linting.yml): update to run black formatting](https://github.com/BerriAI/litellm/commit/611afaf2ab4e688dadb66c0a05774387bc7d957d "ci(test-linting.yml): update to run black formatting") | Mar 31, 2025 |
| [ci\_cd](https://github.com/BerriAI/litellm/tree/main/ci_cd "ci_cd") | [ci\_cd](https://github.com/BerriAI/litellm/tree/main/ci_cd "ci_cd") | [install prisma migration files - connects litellm proxy to litellm's …](https://github.com/BerriAI/litellm/commit/1604f87663f7d6081e99a266b241538bf25ca29f "install prisma migration files - connects litellm proxy to litellm's prisma migration files (#9637)  * build(README.md): initial commit adding a separate folder for additional proxy files. Meant to reduce size of core package  * build(litellm-proxy-extras/): new pip package for storing migration files  allows litellm proxy to use migration files, without adding them to core repo  * build(litellm-proxy-extras/): cleanup pyproject.toml  * build: move prisma migration files inside new proxy extras package  * build(run_migration.py): update script to write to correct folder  * build(proxy_cli.py): load in migration files from litellm-proxy-extras  Closes https://github.com/BerriAI/litellm/issues/9558  * build: add MIT license to litellm-proxy-extras  * test: update test  * fix: fix schema  * bump: version 0.1.0 → 0.1.1  * build(publish-proxy-extras.sh): add script for publishing new proxy-extras version  * build(liccheck.ini): add litellm-proxy-extras to authorized packages  * fix(litellm-proxy-extras/utils.py): move prisma migrate logic inside extra proxy pkg  easier since migrations folder already there  * build(pre-commit-config.yaml): add litellm_proxy_extras to ci tests  * docs(config_settings.md): document new env var  * build(pyproject.toml): bump relevant files when litellm-proxy-extras version changed  * build(pre-commit-config.yaml): run poetry check on litellm-proxy-extras as well") | Mar 29, 2025 |
| [cookbook](https://github.com/BerriAI/litellm/tree/main/cookbook "cookbook") | [cookbook](https://github.com/BerriAI/litellm/tree/main/cookbook "cookbook") | [fix dev release.txt](https://github.com/BerriAI/litellm/commit/5965680176a00f80cc99cbefd9c18730c8a516ae "fix dev release.txt") | Apr 1, 2025 |
| [db\_scripts](https://github.com/BerriAI/litellm/tree/main/db_scripts "db_scripts") | [db\_scripts](https://github.com/BerriAI/litellm/tree/main/db_scripts "db_scripts") | [Litellm dev contributor prs 01 31 2025 (](https://github.com/BerriAI/litellm/commit/91ed05df2962b8eee8492374b048d27cc144d08c "Litellm dev contributor prs 01 31 2025 (#8168)  * Add O3-Mini for Azure and Remove Vision Support (#8161)  * Azure Released O3-mini at the same time as OAI, so i've added support here. Confirmed to work with Sweden Central.  * [FIX] replace cgi for python 3.13 with email.Message as suggested in PEP 594 (#8160)  * Update model_prices_and_context_window.json (#8120)  codestral2501 pricing on vertex_ai  * Fix/db view names (#8119)  * Fix to case sensitive DB Views name  * Fix to case sensitive DB View names  * Added quotes to check query as well  * Added quotes to create view query  * test: handle server error  for flaky test  vertex ai has unstable endpoints  ---------  Co-authored-by: Wanis Elabbar <70503629+elabbarw@users.noreply.github.com> Co-authored-by: Honghua Dong <dhh1995@163.com> Co-authored-by: superpoussin22 <vincent.nadal@orange.fr> Co-authored-by: Miguel Armenta <37154380+ma-armenta@users.noreply.github.com>") [#8168](https://github.com/BerriAI/litellm/pull/8168) [)](https://github.com/BerriAI/litellm/commit/91ed05df2962b8eee8492374b048d27cc144d08c "Litellm dev contributor prs 01 31 2025 (#8168)  * Add O3-Mini for Azure and Remove Vision Support (#8161)  * Azure Released O3-mini at the same time as OAI, so i've added support here. Confirmed to work with Sweden Central.  * [FIX] replace cgi for python 3.13 with email.Message as suggested in PEP 594 (#8160)  * Update model_prices_and_context_window.json (#8120)  codestral2501 pricing on vertex_ai  * Fix/db view names (#8119)  * Fix to case sensitive DB Views name  * Fix to case sensitive DB View names  * Added quotes to check query as well  * Added quotes to create view query  * test: handle server error  for flaky test  vertex ai has unstable endpoints  ---------  Co-authored-by: Wanis Elabbar <70503629+elabbarw@users.noreply.github.com> Co-authored-by: Honghua Dong <dhh1995@163.com> Co-authored-by: superpoussin22 <vincent.nadal@orange.fr> Co-authored-by: Miguel Armenta <37154380+ma-armenta@users.noreply.github.com>") | Feb 1, 2025 |
| [deploy](https://github.com/BerriAI/litellm/tree/main/deploy "deploy") | [deploy](https://github.com/BerriAI/litellm/tree/main/deploy "deploy") | [install prisma migration files - connects litellm proxy to litellm's …](https://github.com/BerriAI/litellm/commit/1604f87663f7d6081e99a266b241538bf25ca29f "install prisma migration files - connects litellm proxy to litellm's prisma migration files (#9637)  * build(README.md): initial commit adding a separate folder for additional proxy files. Meant to reduce size of core package  * build(litellm-proxy-extras/): new pip package for storing migration files  allows litellm proxy to use migration files, without adding them to core repo  * build(litellm-proxy-extras/): cleanup pyproject.toml  * build: move prisma migration files inside new proxy extras package  * build(run_migration.py): update script to write to correct folder  * build(proxy_cli.py): load in migration files from litellm-proxy-extras  Closes https://github.com/BerriAI/litellm/issues/9558  * build: add MIT license to litellm-proxy-extras  * test: update test  * fix: fix schema  * bump: version 0.1.0 → 0.1.1  * build(publish-proxy-extras.sh): add script for publishing new proxy-extras version  * build(liccheck.ini): add litellm-proxy-extras to authorized packages  * fix(litellm-proxy-extras/utils.py): move prisma migrate logic inside extra proxy pkg  easier since migrations folder already there  * build(pre-commit-config.yaml): add litellm_proxy_extras to ci tests  * docs(config_settings.md): document new env var  * build(pyproject.toml): bump relevant files when litellm-proxy-extras version changed  * build(pre-commit-config.yaml): run poetry check on litellm-proxy-extras as well") | Mar 29, 2025 |
| [dist](https://github.com/BerriAI/litellm/tree/main/dist "dist") | [dist](https://github.com/BerriAI/litellm/tree/main/dist "dist") | [Litellm dev 01 10 2025 p2 (](https://github.com/BerriAI/litellm/commit/c4780479a990f532083bdf1e9fa88750eff00098 "Litellm dev 01 10 2025 p2 (#7679)  * test(test_basic_python_version.py): assert all optional dependencies are marked as extras on poetry  Fixes https://github.com/BerriAI/litellm/issues/7677  * docs(secret.md): clarify 'read_and_write' secret manager usage on aws  * docs(secret.md): fix doc  * build(ui/teams.tsx): add edit/delete button for updating user / team membership on ui  allows updating user role to admin on ui  * build(ui/teams.tsx): display edit member component on ui, when edit button on member clicked  * feat(team_endpoints.py): support updating team member role to admin via api endpoints  allows team member to become admin post-add  * build(ui/user_dashboard.tsx): if team admin - show all team keys  Fixes https://github.com/BerriAI/litellm/issues/7650  * test(config.yml): add tomli to ci/cd  * test: don't call python_basic_testing in local testing (covered by python 3.13 testing)") [#7679](https://github.com/BerriAI/litellm/pull/7679) [)](https://github.com/BerriAI/litellm/commit/c4780479a990f532083bdf1e9fa88750eff00098 "Litellm dev 01 10 2025 p2 (#7679)  * test(test_basic_python_version.py): assert all optional dependencies are marked as extras on poetry  Fixes https://github.com/BerriAI/litellm/issues/7677  * docs(secret.md): clarify 'read_and_write' secret manager usage on aws  * docs(secret.md): fix doc  * build(ui/teams.tsx): add edit/delete button for updating user / team membership on ui  allows updating user role to admin on ui  * build(ui/teams.tsx): display edit member component on ui, when edit button on member clicked  * feat(team_endpoints.py): support updating team member role to admin via api endpoints  allows team member to become admin post-add  * build(ui/user_dashboard.tsx): if team admin - show all team keys  Fixes https://github.com/BerriAI/litellm/issues/7650  * test(config.yml): add tomli to ci/cd  * test: don't call python_basic_testing in local testing (covered by python 3.13 testing)") | Jan 10, 2025 |
| [docker](https://github.com/BerriAI/litellm/tree/main/docker "docker") | [docker](https://github.com/BerriAI/litellm/tree/main/docker "docker") | [update redisvl dependency](https://github.com/BerriAI/litellm/commit/7864cd1f76b05fc6f7786a3dc9ea0702f58e92e7 "update redisvl dependency") | Mar 24, 2025 |
| [docs/my-website](https://github.com/BerriAI/litellm/tree/main/docs/my-website "This path skips through empty directories") | [docs/my-website](https://github.com/BerriAI/litellm/tree/main/docs/my-website "This path skips through empty directories") | [docs: cleanup](https://github.com/BerriAI/litellm/commit/bdad9961e332a4c018c40e9f720fd309d7cadc1c "docs: cleanup") | Apr 3, 2025 |
| [enterprise](https://github.com/BerriAI/litellm/tree/main/enterprise "enterprise") | [enterprise](https://github.com/BerriAI/litellm/tree/main/enterprise "enterprise") | [build(pyproject.toml): add new dev dependencies - for type checking (](https://github.com/BerriAI/litellm/commit/9b7ebb6a7d110ffb802698a4a35678967bde2024 "build(pyproject.toml): add new dev dependencies - for type checking (#9631)  * build(pyproject.toml): add new dev dependencies - for type checking  * build: reformat files to fit black  * ci: reformat to fit black  * ci(test-litellm.yml): make tests run clear  * build(pyproject.toml): add ruff  * fix: fix ruff checks  * build(mypy/): fix mypy linting errors  * fix(hashicorp_secret_manager.py): fix passing cert for tls auth  * build(mypy/): resolve all mypy errors  * test: update test  * fix: fix black formatting  * build(pre-commit-config.yaml): use poetry run black  * fix(proxy_server.py): fix linting error  * fix: fix ruff safe representation error") [#…](https://github.com/BerriAI/litellm/pull/9631) | Mar 29, 2025 |
| [litellm-js](https://github.com/BerriAI/litellm/tree/main/litellm-js "litellm-js") | [litellm-js](https://github.com/BerriAI/litellm/tree/main/litellm-js "litellm-js") | [(UI) fix adding Vertex Models (](https://github.com/BerriAI/litellm/commit/4005a51db29161ef3ff39553dbcc58a1b3796fec "(UI) fix adding Vertex Models (#8129)  * fix handleSubmit  * update handleAddModelSubmit  * add jest testing for ui  * add step for running ui unit tests  * add validate json step to add model  * ui jest testing fixes  * update package lock  * ci/cd run again  * fix antd import  * run jest tests first  * fix antd install  * fix ui unit tests  * fix unit test ui") [#8129](https://github.com/BerriAI/litellm/pull/8129) [)](https://github.com/BerriAI/litellm/commit/4005a51db29161ef3ff39553dbcc58a1b3796fec "(UI) fix adding Vertex Models (#8129)  * fix handleSubmit  * update handleAddModelSubmit  * add jest testing for ui  * add step for running ui unit tests  * add validate json step to add model  * ui jest testing fixes  * update package lock  * ci/cd run again  * fix antd import  * run jest tests first  * fix antd install  * fix ui unit tests  * fix unit test ui") | Jan 30, 2025 |
| [litellm-proxy-extras](https://github.com/BerriAI/litellm/tree/main/litellm-proxy-extras "litellm-proxy-extras") | [litellm-proxy-extras](https://github.com/BerriAI/litellm/tree/main/litellm-proxy-extras "litellm-proxy-extras") | [UI (new\_usage.tsx): Report 'total\_tokens' + report success/failure ca…](https://github.com/BerriAI/litellm/commit/62ad84fb6440dcc576ebacdfc8f6fddbbb2efd61 "UI (new_usage.tsx): Report 'total_tokens' + report success/failure calls (#9675)  * feat(internal_user_endpoints.py): return 'total_tokens' in `/user/daily/analytics`  * test(test_internal_user_endpoints.py): add unit test to assert spend metrics and dailyspend metadata always report the same fields  * build(schema.prisma): record success + failure calls to daily user table  allows understanding why model requests might exceed provider requests (e.g. user hit rate limit error)  * fix(internal_user_endpoints.py): report success / failure requests in API  * fix(proxy/utils.py): default to success  status can be missing or none at times for successful requests  * feat(new_usage.tsx): show success/failure calls on UI  * style(new_usage.tsx): ui cleanup  * fix: fix linting error  * fix: fix linting error  * feat(litellm-proxy-extras/): add new migration files") | Mar 31, 2025 |
| [litellm](https://github.com/BerriAI/litellm/tree/main/litellm "litellm") | [litellm](https://github.com/BerriAI/litellm/tree/main/litellm "litellm") | [fix(router.py): support reusable credentials via passthrough router (](https://github.com/BerriAI/litellm/commit/c555c15ad7d65850b3348436d91b910afc1c1907 "fix(router.py): support reusable credentials via passthrough router (#9758)  * fix(router.py): support reusable credentials via passthrough router  enables reusable vertex credentials to be used in passthrough  * test: fix test  * test(test_router_adding_deployments.py): add unit testing") [#…](https://github.com/BerriAI/litellm/pull/9758) | Apr 4, 2025 |
| [tests](https://github.com/BerriAI/litellm/tree/main/tests "tests") | [tests](https://github.com/BerriAI/litellm/tree/main/tests "tests") | [fix(router.py): support reusable credentials via passthrough router (](https://github.com/BerriAI/litellm/commit/c555c15ad7d65850b3348436d91b910afc1c1907 "fix(router.py): support reusable credentials via passthrough router (#9758)  * fix(router.py): support reusable credentials via passthrough router  enables reusable vertex credentials to be used in passthrough  * test: fix test  * test(test_router_adding_deployments.py): add unit testing") [#…](https://github.com/BerriAI/litellm/pull/9758) | Apr 4, 2025 |
| [ui](https://github.com/BerriAI/litellm/tree/main/ui "ui") | [ui](https://github.com/BerriAI/litellm/tree/main/ui "ui") | [ui new build](https://github.com/BerriAI/litellm/commit/cba1dacc7df650089f62419b496dc397214c9cdd "ui new build") | Apr 4, 2025 |
| [.dockerignore](https://github.com/BerriAI/litellm/blob/main/.dockerignore ".dockerignore") | [.dockerignore](https://github.com/BerriAI/litellm/blob/main/.dockerignore ".dockerignore") | [Add back in non root image fixes (](https://github.com/BerriAI/litellm/commit/d4ed98517369b882dea26bfa7e345f519bbbfec4 "Add back in non root image fixes (#7781) (#7795)  * Add back in non root image fixes (#7781)  * Add back in non root image fixes  * Fix dockerfile  * Fix perms  * Add in container structure tests for the nonroot image (#7796)  * feat(helm): add securityContext and pull policy values to migration job (#7652)  * fix(helm): corrected indentation in migration-job.yaml  * feat(helm): add securityContext and pull policy values to migration job  * fix confusing save button label (#7778)  * [integrations/lunary] Improve Lunary documentaiton (#7770)  * update lunary doc  * better title  * tweaks  * Update langchain.md  * Update lunary_integration.md  * Fix wrong URL for internal user invitation (#7762)  * format  * done  * Update instructor tutorial (#7784)  * Add in container structure tests for the nonroot image  ---------  Co-authored-by: Zackeus Bengtsson <32719220+Hexoplon@users.noreply.github.com> Co-authored-by: yujonglee <yujonglee.dev@gmail.com> Co-authored-by: Hugues Chocart <chocart.hugues@icloud.com> Co-authored-by: Nikolaiev Dmytro <dima.nikol.99@gmail.com>  ---------  Co-authored-by: Rajat Vig <rajatvig@users.noreply.github.com> Co-authored-by: Zackeus Bengtsson <32719220+Hexoplon@users.noreply.github.com> Co-authored-by: yujonglee <yujonglee.dev@gmail.com> Co-authored-by: Hugues Chocart <chocart.hugues@icloud.com> Co-authored-by: Nikolaiev Dmytro <dima.nikol.99@gmail.com>") [#7781](https://github.com/BerriAI/litellm/pull/7781) [) (](https://github.com/BerriAI/litellm/commit/d4ed98517369b882dea26bfa7e345f519bbbfec4 "Add back in non root image fixes (#7781) (#7795)  * Add back in non root image fixes (#7781)  * Add back in non root image fixes  * Fix dockerfile  * Fix perms  * Add in container structure tests for the nonroot image (#7796)  * feat(helm): add securityContext and pull policy values to migration job (#7652)  * fix(helm): corrected indentation in migration-job.yaml  * feat(helm): add securityContext and pull policy values to migration job  * fix confusing save button label (#7778)  * [integrations/lunary] Improve Lunary documentaiton (#7770)  * update lunary doc  * better title  * tweaks  * Update langchain.md  * Update lunary_integration.md  * Fix wrong URL for internal user invitation (#7762)  * format  * done  * Update instructor tutorial (#7784)  * Add in container structure tests for the nonroot image  ---------  Co-authored-by: Zackeus Bengtsson <32719220+Hexoplon@users.noreply.github.com> Co-authored-by: yujonglee <yujonglee.dev@gmail.com> Co-authored-by: Hugues Chocart <chocart.hugues@icloud.com> Co-authored-by: Nikolaiev Dmytro <dima.nikol.99@gmail.com>  ---------  Co-authored-by: Rajat Vig <rajatvig@users.noreply.github.com> Co-authored-by: Zackeus Bengtsson <32719220+Hexoplon@users.noreply.github.com> Co-authored-by: yujonglee <yujonglee.dev@gmail.com> Co-authored-by: Hugues Chocart <chocart.hugues@icloud.com> Co-authored-by: Nikolaiev Dmytro <dima.nikol.99@gmail.com>") [#7795](https://github.com/BerriAI/litellm/pull/7795) [)](https://github.com/BerriAI/litellm/commit/d4ed98517369b882dea26bfa7e345f519bbbfec4 "Add back in non root image fixes (#7781) (#7795)  * Add back in non root image fixes (#7781)  * Add back in non root image fixes  * Fix dockerfile  * Fix perms  * Add in container structure tests for the nonroot image (#7796)  * feat(helm): add securityContext and pull policy values to migration job (#7652)  * fix(helm): corrected indentation in migration-job.yaml  * feat(helm): add securityContext and pull policy values to migration job  * fix confusing save button label (#7778)  * [integrations/lunary] Improve Lunary documentaiton (#7770)  * update lunary doc  * better title  * tweaks  * Update langchain.md  * Update lunary_integration.md  * Fix wrong URL for internal user invitation (#7762)  * format  * done  * Update instructor tutorial (#7784)  * Add in container structure tests for the nonroot image  ---------  Co-authored-by: Zackeus Bengtsson <32719220+Hexoplon@users.noreply.github.com> Co-authored-by: yujonglee <yujonglee.dev@gmail.com> Co-authored-by: Hugues Chocart <chocart.hugues@icloud.com> Co-authored-by: Nikolaiev Dmytro <dima.nikol.99@gmail.com>  ---------  Co-authored-by: Rajat Vig <rajatvig@users.noreply.github.com> Co-authored-by: Zackeus Bengtsson <32719220+Hexoplon@users.noreply.github.com> Co-authored-by: yujonglee <yujonglee.dev@gmail.com> Co-authored-by: Hugues Chocart <chocart.hugues@icloud.com> Co-authored-by: Nikolaiev Dmytro <dima.nikol.99@gmail.com>") | Jan 15, 2025 |
| [.env.example](https://github.com/BerriAI/litellm/blob/main/.env.example ".env.example") | [.env.example](https://github.com/BerriAI/litellm/blob/main/.env.example ".env.example") | [expose port & required env vars & instructions for running in dev (](https://github.com/BerriAI/litellm/commit/64ccf4cd6e3379c8c69cd5b762e2a0245fceb2d8 "expose port & required env vars & instructions for running in dev (#8404)") [#8404](https://github.com/BerriAI/litellm/pull/8404) | Feb 8, 2025 |
| [.flake8](https://github.com/BerriAI/litellm/blob/main/.flake8 ".flake8") | [.flake8](https://github.com/BerriAI/litellm/blob/main/.flake8 ".flake8") | [chore: list all ignored flake8 rules explicit](https://github.com/BerriAI/litellm/commit/3aeceb63833f687ee8bf6b536c44f49b31d1bcfd "chore: list all ignored flake8 rules explicit") | Dec 22, 2023 |
| [.git-blame-ignore-revs](https://github.com/BerriAI/litellm/blob/main/.git-blame-ignore-revs ".git-blame-ignore-revs") | [.git-blame-ignore-revs](https://github.com/BerriAI/litellm/blob/main/.git-blame-ignore-revs ".git-blame-ignore-revs") | [Add my commit to .git-blame-ignore-revs](https://github.com/BerriAI/litellm/commit/abe2514ba1a0f26babc3efee2da75af96be37eea "Add my commit to .git-blame-ignore-revs  because I made a lot of fairly mindless changes to pydantic code to fix warnings and I don't want git blame to give people the impression that I know more about this code than I do.") | May 12, 2024 |
| [.gitattributes](https://github.com/BerriAI/litellm/blob/main/.gitattributes ".gitattributes") | [.gitattributes](https://github.com/BerriAI/litellm/blob/main/.gitattributes ".gitattributes") | [ignore ipynbs](https://github.com/BerriAI/litellm/commit/4ce5e8e66d202b4b93eaf13d6cb1b219a23de8bb "ignore ipynbs") | Aug 31, 2023 |
| [.gitignore](https://github.com/BerriAI/litellm/blob/main/.gitignore ".gitignore") | [.gitignore](https://github.com/BerriAI/litellm/blob/main/.gitignore ".gitignore") | [build(.gitignore): update gitignore](https://github.com/BerriAI/litellm/commit/aa2489d74fc5968c7be9add11d9e064170a8edde "build(.gitignore): update gitignore") | Mar 29, 2025 |
| [.pre-commit-config.yaml](https://github.com/BerriAI/litellm/blob/main/.pre-commit-config.yaml ".pre-commit-config.yaml") | [.pre-commit-config.yaml](https://github.com/BerriAI/litellm/blob/main/.pre-commit-config.yaml ".pre-commit-config.yaml") | [install prisma migration files - connects litellm proxy to litellm's …](https://github.com/BerriAI/litellm/commit/1604f87663f7d6081e99a266b241538bf25ca29f "install prisma migration files - connects litellm proxy to litellm's prisma migration files (#9637)  * build(README.md): initial commit adding a separate folder for additional proxy files. Meant to reduce size of core package  * build(litellm-proxy-extras/): new pip package for storing migration files  allows litellm proxy to use migration files, without adding them to core repo  * build(litellm-proxy-extras/): cleanup pyproject.toml  * build: move prisma migration files inside new proxy extras package  * build(run_migration.py): update script to write to correct folder  * build(proxy_cli.py): load in migration files from litellm-proxy-extras  Closes https://github.com/BerriAI/litellm/issues/9558  * build: add MIT license to litellm-proxy-extras  * test: update test  * fix: fix schema  * bump: version 0.1.0 → 0.1.1  * build(publish-proxy-extras.sh): add script for publishing new proxy-extras version  * build(liccheck.ini): add litellm-proxy-extras to authorized packages  * fix(litellm-proxy-extras/utils.py): move prisma migrate logic inside extra proxy pkg  easier since migrations folder already there  * build(pre-commit-config.yaml): add litellm_proxy_extras to ci tests  * docs(config_settings.md): document new env var  * build(pyproject.toml): bump relevant files when litellm-proxy-extras version changed  * build(pre-commit-config.yaml): run poetry check on litellm-proxy-extras as well") | Mar 29, 2025 |
| [Dockerfile](https://github.com/BerriAI/litellm/blob/main/Dockerfile "Dockerfile") | [Dockerfile](https://github.com/BerriAI/litellm/blob/main/Dockerfile "Dockerfile") | [update redisvl dependency](https://github.com/BerriAI/litellm/commit/7864cd1f76b05fc6f7786a3dc9ea0702f58e92e7 "update redisvl dependency") | Mar 24, 2025 |
| [LICENSE](https://github.com/BerriAI/litellm/blob/main/LICENSE "LICENSE") | [LICENSE](https://github.com/BerriAI/litellm/blob/main/LICENSE "LICENSE") | [refactor: creating enterprise folder](https://github.com/BerriAI/litellm/commit/a9e79c8d4645f963c642387e2fef9b8c5474765e "refactor: creating enterprise folder") | Feb 15, 2024 |
| [Makefile](https://github.com/BerriAI/litellm/blob/main/Makefile "Makefile") | [Makefile](https://github.com/BerriAI/litellm/blob/main/Makefile "Makefile") | [fix(proxy\_server.py): get master key from environment, if not set in … (](https://github.com/BerriAI/litellm/commit/0865e52db356c2d37655a2bb1f9278fb509e7c27 "fix(proxy_server.py): get master key from environment, if not set in … (#9617)  * fix(proxy_server.py): get master key from environment, if not set in general settings or general settings not set at all  * test: mark flaky test  * test(test_proxy_server.py): mock prisma client  * ci: add new github workflow for testing just the mock tests  * fix: fix linting error  * ci(conftest.py): add conftest.py to isolate proxy tests  * build(pyproject.toml): add respx to dev dependencies  * build(pyproject.toml): add prisma to dev dependencies  * test: fix mock prompt management tests to use a mock anthropic key  * ci(test-litellm.yml): parallelize mock testing  make it run faster  * build(pyproject.toml): add hypercorn as dev dep  * build(pyproject.toml): separate proxy vs. core dev dependencies  make it easier for non-proxy contributors to run tests locally - e.g. no need to install hypercorn  * ci(test-litellm.yml): pin python version  * test(test_rerank.py): move test - cannot be mocked, requires aws credentials for e2e testing  * ci: add thank you message to ci  * test: add mock env var to test  * test: add autouse to tests  * test: test mock env vars for e2e tests") | Mar 28, 2025 |
| [README.md](https://github.com/BerriAI/litellm/blob/main/README.md "README.md") | [README.md](https://github.com/BerriAI/litellm/blob/main/README.md "README.md") | [Update README.md (](https://github.com/BerriAI/litellm/commit/d60e485c0347da2c3eeb41c3df242fa9749737ec "Update README.md (#9616)") [#9616](https://github.com/BerriAI/litellm/pull/9616) [)](https://github.com/BerriAI/litellm/commit/d60e485c0347da2c3eeb41c3df242fa9749737ec "Update README.md (#9616)") | Mar 28, 2025 |
| [codecov.yaml](https://github.com/BerriAI/litellm/blob/main/codecov.yaml "codecov.yaml") | [codecov.yaml](https://github.com/BerriAI/litellm/blob/main/codecov.yaml "codecov.yaml") | [fix comment](https://github.com/BerriAI/litellm/commit/85f1e5ccfdba71cd75ce758e85268668617da76f "fix comment") | Oct 23, 2024 |
| [docker-compose.yml](https://github.com/BerriAI/litellm/blob/main/docker-compose.yml "docker-compose.yml") | [docker-compose.yml](https://github.com/BerriAI/litellm/blob/main/docker-compose.yml "docker-compose.yml") | [fix docker compose](https://github.com/BerriAI/litellm/commit/b19529a46e7f7ccaaf8e707228a93802b1843d87 "fix docker compose") | Mar 25, 2025 |
| [index.yaml](https://github.com/BerriAI/litellm/blob/main/index.yaml "index.yaml") | [index.yaml](https://github.com/BerriAI/litellm/blob/main/index.yaml "index.yaml") | [add 0.2.3 helm](https://github.com/BerriAI/litellm/commit/f1c39510cb95eb1d29f616c60edbce2783de48b0 "add 0.2.3 helm") | Aug 19, 2024 |
| [mcp\_servers.json](https://github.com/BerriAI/litellm/blob/main/mcp_servers.json "mcp_servers.json") | [mcp\_servers.json](https://github.com/BerriAI/litellm/blob/main/mcp_servers.json "mcp_servers.json") | [mcp servers.json](https://github.com/BerriAI/litellm/commit/ddfa75348507e076b2f19f21b4b3576e323c1bc8 "mcp servers.json") | Mar 21, 2025 |
| [model\_prices\_and\_context\_window.json](https://github.com/BerriAI/litellm/blob/main/model_prices_and_context_window.json "model_prices_and_context_window.json") | [model\_prices\_and\_context\_window.json](https://github.com/BerriAI/litellm/blob/main/model_prices_and_context_window.json "model_prices_and_context_window.json") | [\[Feat\] Add VertexAI gemini-2.0-flash (](https://github.com/BerriAI/litellm/commit/5785600c4ecd2a7dbeab794c70f43f020d0bbeed "[Feat] Add VertexAI gemini-2.0-flash (#9723)") [#9723](https://github.com/BerriAI/litellm/pull/9723) [)](https://github.com/BerriAI/litellm/commit/5785600c4ecd2a7dbeab794c70f43f020d0bbeed "[Feat] Add VertexAI gemini-2.0-flash (#9723)") | Apr 2, 2025 |
| [mypy.ini](https://github.com/BerriAI/litellm/blob/main/mypy.ini "mypy.ini") | [mypy.ini](https://github.com/BerriAI/litellm/blob/main/mypy.ini "mypy.ini") | [Squashed commit of the following: (](https://github.com/BerriAI/litellm/commit/8ee32291e0efe84ee098d8266b49e5d03bed1860 "Squashed commit of the following: (#9709)  commit b12a9892b750f6f838b1049110626261732c8804 Author: Krrish Dholakia <krrishdholakia@gmail.com> Date:   Wed Apr 2 08:09:56 2025 -0700      fix(utils.py): don't modify openai_token_counter  commit 294de3180391f4bb36dcf3ffdd80a0cd387003eb Author: Krrish Dholakia <krrishdholakia@gmail.com> Date:   Mon Mar 24 21:22:40 2025 -0700      fix: fix linting error  commit cb6e9fbe402dacfde36d9d28f872483818ac1f12 Author: Krrish Dholakia <krrishdholakia@gmail.com> Date:   Mon Mar 24 19:52:45 2025 -0700      refactor: complete migration  commit bfc159172dbfffcd9fee47153c554a59cc2758c7 Author: Krrish Dholakia <krrishdholakia@gmail.com> Date:   Mon Mar 24 19:09:59 2025 -0700      refactor: refactor more constants  commit 43ffb6a5583cc3da927042c158ff543af552d2ac Author: Krrish Dholakia <krrishdholakia@gmail.com> Date:   Mon Mar 24 18:45:24 2025 -0700      fix: test  commit 04dbe4310cd588c326194e8e36a0a31cdf0e61fc Author: Krrish Dholakia <krrishdholakia@gmail.com> Date:   Mon Mar 24 18:28:58 2025 -0700      refactor: refactor: move more constants into constants.py  commit 3c26284affeffcec6dd904fdb7ecd3a3d5724660 Author: Krrish Dholakia <krrishdholakia@gmail.com> Date:   Mon Mar 24 18:14:46 2025 -0700      refactor: migrate hardcoded constants out of __init__.py  commit c11e0de69d13f6a97ffed5733cf6008fcdd8e0ee Author: Krrish Dholakia <krrishdholakia@gmail.com> Date:   Mon Mar 24 18:11:21 2025 -0700      build: migrate all constants into constants.py  commit 7882bdc787b21e8e0b1800dd5c1180bbe808d228 Author: Krrish Dholakia <krrishdholakia@gmail.com> Date:   Mon Mar 24 18:07:37 2025 -0700      build: initial test banning hardcoded numbers in repo") [#9709](https://github.com/BerriAI/litellm/pull/9709) [)](https://github.com/BerriAI/litellm/commit/8ee32291e0efe84ee098d8266b49e5d03bed1860 "Squashed commit of the following: (#9709)  commit b12a9892b750f6f838b1049110626261732c8804 Author: Krrish Dholakia <krrishdholakia@gmail.com> Date:   Wed Apr 2 08:09:56 2025 -0700      fix(utils.py): don't modify openai_token_counter  commit 294de3180391f4bb36dcf3ffdd80a0cd387003eb Author: Krrish Dholakia <krrishdholakia@gmail.com> Date:   Mon Mar 24 21:22:40 2025 -0700      fix: fix linting error  commit cb6e9fbe402dacfde36d9d28f872483818ac1f12 Author: Krrish Dholakia <krrishdholakia@gmail.com> Date:   Mon Mar 24 19:52:45 2025 -0700      refactor: complete migration  commit bfc159172dbfffcd9fee47153c554a59cc2758c7 Author: Krrish Dholakia <krrishdholakia@gmail.com> Date:   Mon Mar 24 19:09:59 2025 -0700      refactor: refactor more constants  commit 43ffb6a5583cc3da927042c158ff543af552d2ac Author: Krrish Dholakia <krrishdholakia@gmail.com> Date:   Mon Mar 24 18:45:24 2025 -0700      fix: test  commit 04dbe4310cd588c326194e8e36a0a31cdf0e61fc Author: Krrish Dholakia <krrishdholakia@gmail.com> Date:   Mon Mar 24 18:28:58 2025 -0700      refactor: refactor: move more constants into constants.py  commit 3c26284affeffcec6dd904fdb7ecd3a3d5724660 Author: Krrish Dholakia <krrishdholakia@gmail.com> Date:   Mon Mar 24 18:14:46 2025 -0700      refactor: migrate hardcoded constants out of __init__.py  commit c11e0de69d13f6a97ffed5733cf6008fcdd8e0ee Author: Krrish Dholakia <krrishdholakia@gmail.com> Date:   Mon Mar 24 18:11:21 2025 -0700      build: migrate all constants into constants.py  commit 7882bdc787b21e8e0b1800dd5c1180bbe808d228 Author: Krrish Dholakia <krrishdholakia@gmail.com> Date:   Mon Mar 24 18:07:37 2025 -0700      build: initial test banning hardcoded numbers in repo") | Apr 2, 2025 |
| [package-lock.json](https://github.com/BerriAI/litellm/blob/main/package-lock.json "package-lock.json") | [package-lock.json](https://github.com/BerriAI/litellm/blob/main/package-lock.json "package-lock.json") | [fix(main.py): fix retries being multiplied when using openai sdk (](https://github.com/BerriAI/litellm/commit/ec36353b41db041dcd1746f43f074aedf01d9751 "fix(main.py): fix retries being multiplied when using openai sdk (#7221)  * fix(main.py): fix retries being multiplied when using openai sdk  Closes https://github.com/BerriAI/litellm/pull/7130  * docs(prompt_management.md): add langfuse prompt management doc  * feat(team_endpoints.py): allow teams to add their own models  Enables teams to call their own finetuned models via the proxy  * test: add better enforcement check testing for `/model/new` now that teams can add their own models  * docs(team_model_add.md): tutorial for allowing teams to add their own models  * test: fix test") [#7221](https://github.com/BerriAI/litellm/pull/7221) [)](https://github.com/BerriAI/litellm/commit/ec36353b41db041dcd1746f43f074aedf01d9751 "fix(main.py): fix retries being multiplied when using openai sdk (#7221)  * fix(main.py): fix retries being multiplied when using openai sdk  Closes https://github.com/BerriAI/litellm/pull/7130  * docs(prompt_management.md): add langfuse prompt management doc  * feat(team_endpoints.py): allow teams to add their own models  Enables teams to call their own finetuned models via the proxy  * test: add better enforcement check testing for `/model/new` now that teams can add their own models  * docs(team_model_add.md): tutorial for allowing teams to add their own models  * test: fix test") | Dec 14, 2024 |
| [package.json](https://github.com/BerriAI/litellm/blob/main/package.json "package.json") | [package.json](https://github.com/BerriAI/litellm/blob/main/package.json "package.json") | [fix(main.py): fix retries being multiplied when using openai sdk (](https://github.com/BerriAI/litellm/commit/ec36353b41db041dcd1746f43f074aedf01d9751 "fix(main.py): fix retries being multiplied when using openai sdk (#7221)  * fix(main.py): fix retries being multiplied when using openai sdk  Closes https://github.com/BerriAI/litellm/pull/7130  * docs(prompt_management.md): add langfuse prompt management doc  * feat(team_endpoints.py): allow teams to add their own models  Enables teams to call their own finetuned models via the proxy  * test: add better enforcement check testing for `/model/new` now that teams can add their own models  * docs(team_model_add.md): tutorial for allowing teams to add their own models  * test: fix test") [#7221](https://github.com/BerriAI/litellm/pull/7221) [)](https://github.com/BerriAI/litellm/commit/ec36353b41db041dcd1746f43f074aedf01d9751 "fix(main.py): fix retries being multiplied when using openai sdk (#7221)  * fix(main.py): fix retries being multiplied when using openai sdk  Closes https://github.com/BerriAI/litellm/pull/7130  * docs(prompt_management.md): add langfuse prompt management doc  * feat(team_endpoints.py): allow teams to add their own models  Enables teams to call their own finetuned models via the proxy  * test: add better enforcement check testing for `/model/new` now that teams can add their own models  * docs(team_model_add.md): tutorial for allowing teams to add their own models  * test: fix test") | Dec 14, 2024 |
| [poetry.lock](https://github.com/BerriAI/litellm/blob/main/poetry.lock "poetry.lock") | [poetry.lock](https://github.com/BerriAI/litellm/blob/main/poetry.lock "poetry.lock") | [fix](https://github.com/BerriAI/litellm/commit/d640bc0a00b18960c8dec19b79cb81f89c295cae "fix #8425, passthrough kwargs during acompletion, and unwrap extra_body for openrouter (#9747)") [#8425](https://github.com/BerriAI/litellm/issues/8425) [, passthrough kwargs during acompletion, and unwrap extra\_bo…](https://github.com/BerriAI/litellm/commit/d640bc0a00b18960c8dec19b79cb81f89c295cae "fix #8425, passthrough kwargs during acompletion, and unwrap extra_body for openrouter (#9747)") | Apr 3, 2025 |
| [prometheus.yml](https://github.com/BerriAI/litellm/blob/main/prometheus.yml "prometheus.yml") | [prometheus.yml](https://github.com/BerriAI/litellm/blob/main/prometheus.yml "prometheus.yml") | [build(docker-compose.yml): add prometheus scraper to docker compose](https://github.com/BerriAI/litellm/commit/d9539e518e2d4d82ea2b6ac737de19147790e5ea "build(docker-compose.yml): add prometheus scraper to docker compose  persists prometheus data across restarts") | Jul 24, 2024 |
| [proxy\_server\_config.yaml](https://github.com/BerriAI/litellm/blob/main/proxy_server_config.yaml "proxy_server_config.yaml") | [proxy\_server\_config.yaml](https://github.com/BerriAI/litellm/blob/main/proxy_server_config.yaml "proxy_server_config.yaml") | [test image gen fix in build and test](https://github.com/BerriAI/litellm/commit/8a1023fa2da0c470866685fcc4422fddec8cb8b0 "test image gen fix in build and test") | Apr 2, 2025 |
| [pyproject.toml](https://github.com/BerriAI/litellm/blob/main/pyproject.toml "pyproject.toml") | [pyproject.toml](https://github.com/BerriAI/litellm/blob/main/pyproject.toml "pyproject.toml") | [fix](https://github.com/BerriAI/litellm/commit/d640bc0a00b18960c8dec19b79cb81f89c295cae "fix #8425, passthrough kwargs during acompletion, and unwrap extra_body for openrouter (#9747)") [#8425](https://github.com/BerriAI/litellm/issues/8425) [, passthrough kwargs during acompletion, and unwrap extra\_bo…](https://github.com/BerriAI/litellm/commit/d640bc0a00b18960c8dec19b79cb81f89c295cae "fix #8425, passthrough kwargs during acompletion, and unwrap extra_body for openrouter (#9747)") | Apr 3, 2025 |
| [pyrightconfig.json](https://github.com/BerriAI/litellm/blob/main/pyrightconfig.json "pyrightconfig.json") | [pyrightconfig.json](https://github.com/BerriAI/litellm/blob/main/pyrightconfig.json "pyrightconfig.json") | [Add pyright to ci/cd + Fix remaining type-checking errors (](https://github.com/BerriAI/litellm/commit/fac3b2ee4238e614dc1f077475d9943dbafbc3a4 "Add pyright to ci/cd + Fix remaining type-checking errors (#6082)  * fix: fix type-checking errors  * fix: fix additional type-checking errors  * fix: additional type-checking error fixes  * fix: fix additional type-checking errors  * fix: additional type-check fixes  * fix: fix all type-checking errors + add pyright to ci/cd  * fix: fix incorrect import  * ci(config.yml): use mypy on ci/cd  * fix: fix type-checking errors in utils.py  * fix: fix all type-checking errors on main.py  * fix: fix mypy linting errors  * fix(anthropic/cost_calculator.py): fix linting errors  * fix: fix mypy linting errors  * fix: fix linting errors") [#6082](https://github.com/BerriAI/litellm/pull/6082) [)](https://github.com/BerriAI/litellm/commit/fac3b2ee4238e614dc1f077475d9943dbafbc3a4 "Add pyright to ci/cd + Fix remaining type-checking errors (#6082)  * fix: fix type-checking errors  * fix: fix additional type-checking errors  * fix: additional type-checking error fixes  * fix: fix additional type-checking errors  * fix: additional type-check fixes  * fix: fix all type-checking errors + add pyright to ci/cd  * fix: fix incorrect import  * ci(config.yml): use mypy on ci/cd  * fix: fix type-checking errors in utils.py  * fix: fix all type-checking errors on main.py  * fix: fix mypy linting errors  * fix(anthropic/cost_calculator.py): fix linting errors  * fix: fix mypy linting errors  * fix: fix linting errors") | Oct 5, 2024 |
| [render.yaml](https://github.com/BerriAI/litellm/blob/main/render.yaml "render.yaml") | [render.yaml](https://github.com/BerriAI/litellm/blob/main/render.yaml "render.yaml") | [build(render.yaml): fix health check route](https://github.com/BerriAI/litellm/commit/f8a82f57793edbe9cb903cfc9b0c0bed0f20e1e4 "build(render.yaml): fix health check route") | May 24, 2024 |
| [requirements.txt](https://github.com/BerriAI/litellm/blob/main/requirements.txt "requirements.txt") | [requirements.txt](https://github.com/BerriAI/litellm/blob/main/requirements.txt "requirements.txt") | [UI (new\_usage.tsx): Report 'total\_tokens' + report success/failure ca…](https://github.com/BerriAI/litellm/commit/62ad84fb6440dcc576ebacdfc8f6fddbbb2efd61 "UI (new_usage.tsx): Report 'total_tokens' + report success/failure calls (#9675)  * feat(internal_user_endpoints.py): return 'total_tokens' in `/user/daily/analytics`  * test(test_internal_user_endpoints.py): add unit test to assert spend metrics and dailyspend metadata always report the same fields  * build(schema.prisma): record success + failure calls to daily user table  allows understanding why model requests might exceed provider requests (e.g. user hit rate limit error)  * fix(internal_user_endpoints.py): report success / failure requests in API  * fix(proxy/utils.py): default to success  status can be missing or none at times for successful requests  * feat(new_usage.tsx): show success/failure calls on UI  * style(new_usage.tsx): ui cleanup  * fix: fix linting error  * fix: fix linting error  * feat(litellm-proxy-extras/): add new migration files") | Mar 31, 2025 |
| [ruff.toml](https://github.com/BerriAI/litellm/blob/main/ruff.toml "ruff.toml") | [ruff.toml](https://github.com/BerriAI/litellm/blob/main/ruff.toml "ruff.toml") | [(code quality) run ruff rule to ban unused imports (](https://github.com/BerriAI/litellm/commit/c7f14e936a59586b0b4fe215dfea03650ad9b0cf "(code quality) run ruff rule to ban unused imports  (#7313)  * remove unused imports  * fix AmazonConverseConfig  * fix test  * fix import  * ruff check fixes  * test fixes  * fix testing  * fix imports") [#7313](https://github.com/BerriAI/litellm/pull/7313) [)](https://github.com/BerriAI/litellm/commit/c7f14e936a59586b0b4fe215dfea03650ad9b0cf "(code quality) run ruff rule to ban unused imports  (#7313)  * remove unused imports  * fix AmazonConverseConfig  * fix test  * fix import  * ruff check fixes  * test fixes  * fix testing  * fix imports") | Dec 19, 2024 |
| [schema.prisma](https://github.com/BerriAI/litellm/blob/main/schema.prisma "schema.prisma") | [schema.prisma](https://github.com/BerriAI/litellm/blob/main/schema.prisma "schema.prisma") | [UI (new\_usage.tsx): Report 'total\_tokens' + report success/failure ca…](https://github.com/BerriAI/litellm/commit/62ad84fb6440dcc576ebacdfc8f6fddbbb2efd61 "UI (new_usage.tsx): Report 'total_tokens' + report success/failure calls (#9675)  * feat(internal_user_endpoints.py): return 'total_tokens' in `/user/daily/analytics`  * test(test_internal_user_endpoints.py): add unit test to assert spend metrics and dailyspend metadata always report the same fields  * build(schema.prisma): record success + failure calls to daily user table  allows understanding why model requests might exceed provider requests (e.g. user hit rate limit error)  * fix(internal_user_endpoints.py): report success / failure requests in API  * fix(proxy/utils.py): default to success  status can be missing or none at times for successful requests  * feat(new_usage.tsx): show success/failure calls on UI  * style(new_usage.tsx): ui cleanup  * fix: fix linting error  * fix: fix linting error  * feat(litellm-proxy-extras/): add new migration files") | Mar 31, 2025 |
| [security.md](https://github.com/BerriAI/litellm/blob/main/security.md "security.md") | [security.md](https://github.com/BerriAI/litellm/blob/main/security.md "security.md") | [docs(security.md): Adds security.md file to project root](https://github.com/BerriAI/litellm/commit/41114f1c25a47b309b193835ebca47a42340e5f9 "docs(security.md): Adds security.md file to project root  Closes https://github.com/BerriAI/litellm/issues/5473") | Sep 2, 2024 |
| View all files |

## Repository files navigation

# 🚅 LiteLLM

[Permalink:\
        🚅 LiteLLM\
    ](https://github.com/BerriAI/litellm#---------litellm----)

[![Deploy to Render](https://camo.githubusercontent.com/a103822afe1d58c7da6beafbc0c65bb7b8d622dd193dded1b45b3c0ad6466d82/68747470733a2f2f72656e6465722e636f6d2f696d616765732f6465706c6f792d746f2d72656e6465722d627574746f6e2e737667)](https://render.com/deploy?repo=https://github.com/BerriAI/litellm)[![Deploy on Railway](https://camo.githubusercontent.com/e4002051668809c220b10ad92ddd6fb87f365d8cd4ff470e0aeca3bc5b05450e/68747470733a2f2f7261696c7761792e6170702f627574746f6e2e737667)](https://railway.app/template/HLP0Ub?referralCode=jch2ME)

Call all LLM APIs using the OpenAI format \[Bedrock, Huggingface, VertexAI, TogetherAI, Azure, OpenAI, Groq etc.\]


#### [LiteLLM Proxy Server (LLM Gateway)](https://docs.litellm.ai/docs/simple_proxy) \| [Hosted Proxy (Preview)](https://docs.litellm.ai/docs/hosted) \| [Enterprise Tier](https://docs.litellm.ai/docs/enterprise)

[Permalink: LiteLLM Proxy Server (LLM Gateway) |  Hosted Proxy (Preview) | Enterprise Tier](https://github.com/BerriAI/litellm#litellm-proxy-server-llm-gateway---hosted-proxy-preview--enterprise-tier)

#### [![PyPI Version](https://camo.githubusercontent.com/de190803172c4d35f85e73a0f4eec265b5029bb0ad250f402aac9ca1bd73bd79/68747470733a2f2f696d672e736869656c64732e696f2f707970692f762f6c6974656c6c6d2e737667)](https://pypi.org/project/litellm/)[![Y Combinator W23](https://camo.githubusercontent.com/e1e0029e353d103690da84a20e88b7051eebbcdede2a1b35d9d1b78b0b0295cf/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f59253230436f6d62696e61746f722d5732332d6f72616e67653f7374796c653d666c61742d737175617265)](https://www.ycombinator.com/companies/berriai)[![Whatsapp](https://camo.githubusercontent.com/78382e0d13839fedd81996b3e7cbecea33222e5ea36d54d07455a93dfd68e5d7/68747470733a2f2f696d672e736869656c64732e696f2f7374617469632f76313f6c6162656c3d436861742532306f6e266d6573736167653d576861747341707026636f6c6f723d73756363657373266c6f676f3d5768617473417070267374796c653d666c61742d737175617265)](https://wa.link/huol9n)[![Discord](https://camo.githubusercontent.com/bcba2d72b7345e8de3adc1f330b340b72f37842dd275a91c4f31154e23cc8cd0/68747470733a2f2f696d672e736869656c64732e696f2f7374617469632f76313f6c6162656c3d436861742532306f6e266d6573736167653d446973636f726426636f6c6f723d626c7565266c6f676f3d446973636f7264267374796c653d666c61742d737175617265)](https://discord.gg/wuPM9dRgDw)

[Permalink: ](https://github.com/BerriAI/litellm#----------------------------------------------------------------)

LiteLLM manages:

- Translate inputs to provider's `completion`, `embedding`, and `image_generation` endpoints
- [Consistent output](https://docs.litellm.ai/docs/completion/output), text responses will always be available at `['choices'][0]['message']['content']`
- Retry/fallback logic across multiple deployments (e.g. Azure/OpenAI) - [Router](https://docs.litellm.ai/docs/routing)
- Set Budgets & Rate limits per project, api key, model [LiteLLM Proxy Server (LLM Gateway)](https://docs.litellm.ai/docs/simple_proxy)

[**Jump to LiteLLM Proxy (LLM Gateway) Docs**](https://github.com/BerriAI/litellm?tab=readme-ov-file#openai-proxy---docs)

[**Jump to Supported LLM Providers**](https://github.com/BerriAI/litellm?tab=readme-ov-file#supported-providers-docs)

🚨 **Stable Release:** Use docker images with the `-stable` tag. These have undergone 12 hour load tests, before being published. [More information about the release cycle here](https://docs.litellm.ai/docs/proxy/release_cycle)

Support for more providers. Missing a provider or LLM Platform, raise a [feature request](https://github.com/BerriAI/litellm/issues/new?assignees=&labels=enhancement&projects=&template=feature_request.yml&title=%5BFeature%5D%3A+).

# Usage ( [**Docs**](https://docs.litellm.ai/docs/))

[Permalink: Usage (Docs)](https://github.com/BerriAI/litellm#usage-docs)

Important

LiteLLM v1.0.0 now requires `openai>=1.0.0`. Migration guide [here](https://docs.litellm.ai/docs/migration)

LiteLLM v1.40.14+ now requires `pydantic>=2.0.0`. No changes required.

[![Open In Colab](https://camo.githubusercontent.com/96889048f8a9014fdeba2a891f97150c6aac6e723f5190236b10215a97ed41f3/68747470733a2f2f636f6c61622e72657365617263682e676f6f676c652e636f6d2f6173736574732f636f6c61622d62616467652e737667)](https://colab.research.google.com/github/BerriAI/litellm/blob/main/cookbook/liteLLM_Getting_Started.ipynb)

```
pip install litellm
```

```
from litellm import completion
import os

## set ENV variables
os.environ["OPENAI_API_KEY"] = "your-openai-key"
os.environ["ANTHROPIC_API_KEY"] = "your-anthropic-key"

messages = [{ "content": "Hello, how are you?","role": "user"}]

# openai call
response = completion(model="openai/gpt-4o", messages=messages)

# anthropic call
response = completion(model="anthropic/claude-3-sonnet-20240229", messages=messages)
print(response)
```

### Response (OpenAI Format)

[Permalink: Response (OpenAI Format)](https://github.com/BerriAI/litellm#response-openai-format)

```
{
    "id": "chatcmpl-565d891b-a42e-4c39-8d14-82a1f5208885",
    "created": 1734366691,
    "model": "claude-3-sonnet-20240229",
    "object": "chat.completion",
    "system_fingerprint": null,
    "choices": [\
        {\
            "finish_reason": "stop",\
            "index": 0,\
            "message": {\
                "content": "Hello! As an AI language model, I don't have feelings, but I'm operating properly and ready to assist you with any questions or tasks you may have. How can I help you today?",\
                "role": "assistant",\
                "tool_calls": null,\
                "function_call": null\
            }\
        }\
    ],
    "usage": {
        "completion_tokens": 43,
        "prompt_tokens": 13,
        "total_tokens": 56,
        "completion_tokens_details": null,
        "prompt_tokens_details": {
            "audio_tokens": null,
            "cached_tokens": 0
        },
        "cache_creation_input_tokens": 0,
        "cache_read_input_tokens": 0
    }
}
```

Call any model supported by a provider, with `model=<provider_name>/<model_name>`. There might be provider-specific details here, so refer to [provider docs for more information](https://docs.litellm.ai/docs/providers)

## Async ( [Docs](https://docs.litellm.ai/docs/completion/stream\#async-completion))

[Permalink: Async (Docs)](https://github.com/BerriAI/litellm#async-docs)

```
from litellm import acompletion
import asyncio

async def test_get_response():
    user_message = "Hello, how are you?"
    messages = [{"content": user_message, "role": "user"}]
    response = await acompletion(model="openai/gpt-4o", messages=messages)
    return response

response = asyncio.run(test_get_response())
print(response)
```

## Streaming ( [Docs](https://docs.litellm.ai/docs/completion/stream))

[Permalink: Streaming (Docs)](https://github.com/BerriAI/litellm#streaming-docs)

liteLLM supports streaming the model response back, pass `stream=True` to get a streaming iterator in response.

Streaming is supported for all models (Bedrock, Huggingface, TogetherAI, Azure, OpenAI, etc.)

```
from litellm import completion
response = completion(model="openai/gpt-4o", messages=messages, stream=True)
for part in response:
    print(part.choices[0].delta.content or "")

# claude 2
response = completion('anthropic/claude-3-sonnet-20240229', messages, stream=True)
for part in response:
    print(part)
```

### Response chunk (OpenAI Format)

[Permalink: Response chunk (OpenAI Format)](https://github.com/BerriAI/litellm#response-chunk-openai-format)

```
{
    "id": "chatcmpl-2be06597-eb60-4c70-9ec5-8cd2ab1b4697",
    "created": 1734366925,
    "model": "claude-3-sonnet-20240229",
    "object": "chat.completion.chunk",
    "system_fingerprint": null,
    "choices": [\
        {\
            "finish_reason": null,\
            "index": 0,\
            "delta": {\
                "content": "Hello",\
                "role": "assistant",\
                "function_call": null,\
                "tool_calls": null,\
                "audio": null\
            },\
            "logprobs": null\
        }\
    ]
}
```

## Logging Observability ( [Docs](https://docs.litellm.ai/docs/observability/callbacks))

[Permalink: Logging Observability (Docs)](https://github.com/BerriAI/litellm#logging-observability-docs)

LiteLLM exposes pre defined callbacks to send data to Lunary, MLflow, Langfuse, DynamoDB, s3 Buckets, Helicone, Promptlayer, Traceloop, Athina, Slack

```
from litellm import completion

## set env variables for logging tools (when using MLflow, no API key set up is required)
os.environ["LUNARY_PUBLIC_KEY"] = "your-lunary-public-key"
os.environ["HELICONE_API_KEY"] = "your-helicone-auth-key"
os.environ["LANGFUSE_PUBLIC_KEY"] = ""
os.environ["LANGFUSE_SECRET_KEY"] = ""
os.environ["ATHINA_API_KEY"] = "your-athina-api-key"

os.environ["OPENAI_API_KEY"] = "your-openai-key"

# set callbacks
litellm.success_callback = ["lunary", "mlflow", "langfuse", "athina", "helicone"] # log input/output to lunary, langfuse, supabase, athina, helicone etc

#openai call
response = completion(model="openai/gpt-4o", messages=[{"role": "user", "content": "Hi 👋 - i'm openai"}])
```

# LiteLLM Proxy Server (LLM Gateway) - ( [Docs](https://docs.litellm.ai/docs/simple_proxy))

[Permalink: LiteLLM Proxy Server (LLM Gateway) - (Docs)](https://github.com/BerriAI/litellm#litellm-proxy-server-llm-gateway---docs)

Track spend + Load Balance across multiple projects

[Hosted Proxy (Preview)](https://docs.litellm.ai/docs/hosted)

The proxy provides:

1. [Hooks for auth](https://docs.litellm.ai/docs/proxy/virtual_keys#custom-auth)
2. [Hooks for logging](https://docs.litellm.ai/docs/proxy/logging#step-1---create-your-custom-litellm-callback-class)
3. [Cost tracking](https://docs.litellm.ai/docs/proxy/virtual_keys#tracking-spend)
4. [Rate Limiting](https://docs.litellm.ai/docs/proxy/users#set-rate-limits)

## 📖 Proxy Endpoints - [Swagger Docs](https://litellm-api.up.railway.app/)

[Permalink: 📖 Proxy Endpoints - Swagger Docs](https://github.com/BerriAI/litellm#-proxy-endpoints---swagger-docs)

## Quick Start Proxy - CLI

[Permalink: Quick Start Proxy - CLI](https://github.com/BerriAI/litellm#quick-start-proxy---cli)

```
pip install 'litellm[proxy]'
```

### Step 1: Start litellm proxy

[Permalink: Step 1: Start litellm proxy](https://github.com/BerriAI/litellm#step-1-start-litellm-proxy)

```
$ litellm --model huggingface/bigcode/starcoder

#INFO: Proxy running on http://0.0.0.0:4000
```

### Step 2: Make ChatCompletions Request to Proxy

[Permalink: Step 2: Make ChatCompletions Request to Proxy](https://github.com/BerriAI/litellm#step-2-make-chatcompletions-request-to-proxy)

Important

💡 [Use LiteLLM Proxy with Langchain (Python, JS), OpenAI SDK (Python, JS) Anthropic SDK, Mistral SDK, LlamaIndex, Instructor, Curl](https://docs.litellm.ai/docs/proxy/user_keys)

```
import openai # openai v1.0.0+
client = openai.OpenAI(api_key="anything",base_url="http://0.0.0.0:4000") # set proxy to base_url
# request sent to model set on litellm proxy, `litellm --model`
response = client.chat.completions.create(model="gpt-3.5-turbo", messages = [\
    {\
        "role": "user",\
        "content": "this is a test request, write a short poem"\
    }\
])

print(response)
```

## Proxy Key Management ( [Docs](https://docs.litellm.ai/docs/proxy/virtual_keys))

[Permalink: Proxy Key Management (Docs)](https://github.com/BerriAI/litellm#proxy-key-management-docs)

Connect the proxy with a Postgres DB to create proxy keys

```
# Get the code
git clone https://github.com/BerriAI/litellm

# Go to folder
cd litellm

# Add the master key - you can change this after setup
echo 'LITELLM_MASTER_KEY="sk-1234"' > .env

# Add the litellm salt key - you cannot change this after adding a model
# It is used to encrypt / decrypt your LLM API Key credentials
# We recommend - https://1password.com/password-generator/
# password generator to get a random hash for litellm salt key
echo 'LITELLM_SALT_KEY="sk-1234"' > .env

source .env

# Start
docker-compose up
```

UI on `/ui` on your proxy server
[![ui_3](https://private-user-images.githubusercontent.com/29436595/302077487-47c97d5e-b9be-4839-b28c-43d7f4f10033.gif?jwt=eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJpc3MiOiJnaXRodWIuY29tIiwiYXVkIjoicmF3LmdpdGh1YnVzZXJjb250ZW50LmNvbSIsImtleSI6ImtleTUiLCJleHAiOjE3NDM4MjI3MTMsIm5iZiI6MTc0MzgyMjQxMywicGF0aCI6Ii8yOTQzNjU5NS8zMDIwNzc0ODctNDdjOTdkNWUtYjliZS00ODM5LWIyOGMtNDNkN2Y0ZjEwMDMzLmdpZj9YLUFtei1BbGdvcml0aG09QVdTNC1ITUFDLVNIQTI1NiZYLUFtei1DcmVkZW50aWFsPUFLSUFWQ09EWUxTQTUzUFFLNFpBJTJGMjAyNTA0MDUlMkZ1cy1lYXN0LTElMkZzMyUyRmF3czRfcmVxdWVzdCZYLUFtei1EYXRlPTIwMjUwNDA1VDAzMDY1M1omWC1BbXotRXhwaXJlcz0zMDAmWC1BbXotU2lnbmF0dXJlPWYxZjU0YWZlYjU0OGNlY2E0YmE4ZWQ0M2I0N2I3N2Q4OWY2MDgwMjBiNzc3OGY2ZmUzNGQxYjcyYmVmMThlZjQmWC1BbXotU2lnbmVkSGVhZGVycz1ob3N0In0.goRrlrnC6x8gi41d9ov4BTVQIuAoRkl_BSoqITuHnFE)](https://private-user-images.githubusercontent.com/29436595/302077487-47c97d5e-b9be-4839-b28c-43d7f4f10033.gif?jwt=eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJpc3MiOiJnaXRodWIuY29tIiwiYXVkIjoicmF3LmdpdGh1YnVzZXJjb250ZW50LmNvbSIsImtleSI6ImtleTUiLCJleHAiOjE3NDM4MjI3MTMsIm5iZiI6MTc0MzgyMjQxMywicGF0aCI6Ii8yOTQzNjU5NS8zMDIwNzc0ODctNDdjOTdkNWUtYjliZS00ODM5LWIyOGMtNDNkN2Y0ZjEwMDMzLmdpZj9YLUFtei1BbGdvcml0aG09QVdTNC1ITUFDLVNIQTI1NiZYLUFtei1DcmVkZW50aWFsPUFLSUFWQ09EWUxTQTUzUFFLNFpBJTJGMjAyNTA0MDUlMkZ1cy1lYXN0LTElMkZzMyUyRmF3czRfcmVxdWVzdCZYLUFtei1EYXRlPTIwMjUwNDA1VDAzMDY1M1omWC1BbXotRXhwaXJlcz0zMDAmWC1BbXotU2lnbmF0dXJlPWYxZjU0YWZlYjU0OGNlY2E0YmE4ZWQ0M2I0N2I3N2Q4OWY2MDgwMjBiNzc3OGY2ZmUzNGQxYjcyYmVmMThlZjQmWC1BbXotU2lnbmVkSGVhZGVycz1ob3N0In0.goRrlrnC6x8gi41d9ov4BTVQIuAoRkl_BSoqITuHnFE)[![ui_3](https://private-user-images.githubusercontent.com/29436595/302077487-47c97d5e-b9be-4839-b28c-43d7f4f10033.gif?jwt=eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJpc3MiOiJnaXRodWIuY29tIiwiYXVkIjoicmF3LmdpdGh1YnVzZXJjb250ZW50LmNvbSIsImtleSI6ImtleTUiLCJleHAiOjE3NDM4MjI3MTMsIm5iZiI6MTc0MzgyMjQxMywicGF0aCI6Ii8yOTQzNjU5NS8zMDIwNzc0ODctNDdjOTdkNWUtYjliZS00ODM5LWIyOGMtNDNkN2Y0ZjEwMDMzLmdpZj9YLUFtei1BbGdvcml0aG09QVdTNC1ITUFDLVNIQTI1NiZYLUFtei1DcmVkZW50aWFsPUFLSUFWQ09EWUxTQTUzUFFLNFpBJTJGMjAyNTA0MDUlMkZ1cy1lYXN0LTElMkZzMyUyRmF3czRfcmVxdWVzdCZYLUFtei1EYXRlPTIwMjUwNDA1VDAzMDY1M1omWC1BbXotRXhwaXJlcz0zMDAmWC1BbXotU2lnbmF0dXJlPWYxZjU0YWZlYjU0OGNlY2E0YmE4ZWQ0M2I0N2I3N2Q4OWY2MDgwMjBiNzc3OGY2ZmUzNGQxYjcyYmVmMThlZjQmWC1BbXotU2lnbmVkSGVhZGVycz1ob3N0In0.goRrlrnC6x8gi41d9ov4BTVQIuAoRkl_BSoqITuHnFE)](https://private-user-images.githubusercontent.com/29436595/302077487-47c97d5e-b9be-4839-b28c-43d7f4f10033.gif?jwt=eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJpc3MiOiJnaXRodWIuY29tIiwiYXVkIjoicmF3LmdpdGh1YnVzZXJjb250ZW50LmNvbSIsImtleSI6ImtleTUiLCJleHAiOjE3NDM4MjI3MTMsIm5iZiI6MTc0MzgyMjQxMywicGF0aCI6Ii8yOTQzNjU5NS8zMDIwNzc0ODctNDdjOTdkNWUtYjliZS00ODM5LWIyOGMtNDNkN2Y0ZjEwMDMzLmdpZj9YLUFtei1BbGdvcml0aG09QVdTNC1ITUFDLVNIQTI1NiZYLUFtei1DcmVkZW50aWFsPUFLSUFWQ09EWUxTQTUzUFFLNFpBJTJGMjAyNTA0MDUlMkZ1cy1lYXN0LTElMkZzMyUyRmF3czRfcmVxdWVzdCZYLUFtei1EYXRlPTIwMjUwNDA1VDAzMDY1M1omWC1BbXotRXhwaXJlcz0zMDAmWC1BbXotU2lnbmF0dXJlPWYxZjU0YWZlYjU0OGNlY2E0YmE4ZWQ0M2I0N2I3N2Q4OWY2MDgwMjBiNzc3OGY2ZmUzNGQxYjcyYmVmMThlZjQmWC1BbXotU2lnbmVkSGVhZGVycz1ob3N0In0.goRrlrnC6x8gi41d9ov4BTVQIuAoRkl_BSoqITuHnFE)[Open in new window](https://private-user-images.githubusercontent.com/29436595/302077487-47c97d5e-b9be-4839-b28c-43d7f4f10033.gif?jwt=eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJpc3MiOiJnaXRodWIuY29tIiwiYXVkIjoicmF3LmdpdGh1YnVzZXJjb250ZW50LmNvbSIsImtleSI6ImtleTUiLCJleHAiOjE3NDM4MjI3MTMsIm5iZiI6MTc0MzgyMjQxMywicGF0aCI6Ii8yOTQzNjU5NS8zMDIwNzc0ODctNDdjOTdkNWUtYjliZS00ODM5LWIyOGMtNDNkN2Y0ZjEwMDMzLmdpZj9YLUFtei1BbGdvcml0aG09QVdTNC1ITUFDLVNIQTI1NiZYLUFtei1DcmVkZW50aWFsPUFLSUFWQ09EWUxTQTUzUFFLNFpBJTJGMjAyNTA0MDUlMkZ1cy1lYXN0LTElMkZzMyUyRmF3czRfcmVxdWVzdCZYLUFtei1EYXRlPTIwMjUwNDA1VDAzMDY1M1omWC1BbXotRXhwaXJlcz0zMDAmWC1BbXotU2lnbmF0dXJlPWYxZjU0YWZlYjU0OGNlY2E0YmE4ZWQ0M2I0N2I3N2Q4OWY2MDgwMjBiNzc3OGY2ZmUzNGQxYjcyYmVmMThlZjQmWC1BbXotU2lnbmVkSGVhZGVycz1ob3N0In0.goRrlrnC6x8gi41d9ov4BTVQIuAoRkl_BSoqITuHnFE)

Set budgets and rate limits across multiple projects
`POST /key/generate`

### Request

[Permalink: Request](https://github.com/BerriAI/litellm#request)

```
curl 'http://0.0.0.0:4000/key/generate' \
--header 'Authorization: Bearer sk-1234' \
--header 'Content-Type: application/json' \
--data-raw '{"models": ["gpt-3.5-turbo", "gpt-4", "claude-2"], "duration": "20m","metadata": {"user": "ishaan@berri.ai", "team": "core-infra"}}'
```

### Expected Response

[Permalink: Expected Response](https://github.com/BerriAI/litellm#expected-response)

```
{
    "key": "sk-kdEXbIqZRwEeEiHwdg7sFA", # Bearer token
    "expires": "2023-11-19T01:38:25.838000+00:00" # datetime object
}
```

## Supported Providers ( [Docs](https://docs.litellm.ai/docs/providers))

[Permalink: Supported Providers (Docs)](https://github.com/BerriAI/litellm#supported-providers-docs)

| Provider | [Completion](https://docs.litellm.ai/docs/#basic-usage) | [Streaming](https://docs.litellm.ai/docs/completion/stream#streaming-responses) | [Async Completion](https://docs.litellm.ai/docs/completion/stream#async-completion) | [Async Streaming](https://docs.litellm.ai/docs/completion/stream#async-streaming) | [Async Embedding](https://docs.litellm.ai/docs/embedding/supported_embedding) | [Async Image Generation](https://docs.litellm.ai/docs/image_generation) |
| --- | --- | --- | --- | --- | --- | --- |
| [openai](https://docs.litellm.ai/docs/providers/openai) | ✅ | ✅ | ✅ | ✅ | ✅ | ✅ |
| [azure](https://docs.litellm.ai/docs/providers/azure) | ✅ | ✅ | ✅ | ✅ | ✅ | ✅ |
| [AI/ML API](https://docs.litellm.ai/docs/providers/aiml) | ✅ | ✅ | ✅ | ✅ | ✅ | ✅ |
| [aws - sagemaker](https://docs.litellm.ai/docs/providers/aws_sagemaker) | ✅ | ✅ | ✅ | ✅ | ✅ |  |
| [aws - bedrock](https://docs.litellm.ai/docs/providers/bedrock) | ✅ | ✅ | ✅ | ✅ | ✅ |  |
| [google - vertex\_ai](https://docs.litellm.ai/docs/providers/vertex) | ✅ | ✅ | ✅ | ✅ | ✅ | ✅ |
| [google - palm](https://docs.litellm.ai/docs/providers/palm) | ✅ | ✅ | ✅ | ✅ |  |  |
| [google AI Studio - gemini](https://docs.litellm.ai/docs/providers/gemini) | ✅ | ✅ | ✅ | ✅ |  |  |
| [mistral ai api](https://docs.litellm.ai/docs/providers/mistral) | ✅ | ✅ | ✅ | ✅ | ✅ |  |
| [cloudflare AI Workers](https://docs.litellm.ai/docs/providers/cloudflare_workers) | ✅ | ✅ | ✅ | ✅ |  |  |
| [cohere](https://docs.litellm.ai/docs/providers/cohere) | ✅ | ✅ | ✅ | ✅ | ✅ |  |
| [anthropic](https://docs.litellm.ai/docs/providers/anthropic) | ✅ | ✅ | ✅ | ✅ |  |  |
| [empower](https://docs.litellm.ai/docs/providers/empower) | ✅ | ✅ | ✅ | ✅ |  |  |
| [huggingface](https://docs.litellm.ai/docs/providers/huggingface) | ✅ | ✅ | ✅ | ✅ | ✅ |  |
| [replicate](https://docs.litellm.ai/docs/providers/replicate) | ✅ | ✅ | ✅ | ✅ |  |  |
| [together\_ai](https://docs.litellm.ai/docs/providers/togetherai) | ✅ | ✅ | ✅ | ✅ |  |  |
| [openrouter](https://docs.litellm.ai/docs/providers/openrouter) | ✅ | ✅ | ✅ | ✅ |  |  |
| [ai21](https://docs.litellm.ai/docs/providers/ai21) | ✅ | ✅ | ✅ | ✅ |  |  |
| [baseten](https://docs.litellm.ai/docs/providers/baseten) | ✅ | ✅ | ✅ | ✅ |  |  |
| [vllm](https://docs.litellm.ai/docs/providers/vllm) | ✅ | ✅ | ✅ | ✅ |  |  |
| [nlp\_cloud](https://docs.litellm.ai/docs/providers/nlp_cloud) | ✅ | ✅ | ✅ | ✅ |  |  |
| [aleph alpha](https://docs.litellm.ai/docs/providers/aleph_alpha) | ✅ | ✅ | ✅ | ✅ |  |  |
| [petals](https://docs.litellm.ai/docs/providers/petals) | ✅ | ✅ | ✅ | ✅ |  |  |
| [ollama](https://docs.litellm.ai/docs/providers/ollama) | ✅ | ✅ | ✅ | ✅ | ✅ |  |
| [deepinfra](https://docs.litellm.ai/docs/providers/deepinfra) | ✅ | ✅ | ✅ | ✅ |  |  |
| [perplexity-ai](https://docs.litellm.ai/docs/providers/perplexity) | ✅ | ✅ | ✅ | ✅ |  |  |
| [Groq AI](https://docs.litellm.ai/docs/providers/groq) | ✅ | ✅ | ✅ | ✅ |  |  |
| [Deepseek](https://docs.litellm.ai/docs/providers/deepseek) | ✅ | ✅ | ✅ | ✅ |  |  |
| [anyscale](https://docs.litellm.ai/docs/providers/anyscale) | ✅ | ✅ | ✅ | ✅ |  |  |
| [IBM - watsonx.ai](https://docs.litellm.ai/docs/providers/watsonx) | ✅ | ✅ | ✅ | ✅ | ✅ |  |
| [voyage ai](https://docs.litellm.ai/docs/providers/voyage) |  |  |  |  | ✅ |  |
| [xinference \[Xorbits Inference\]](https://docs.litellm.ai/docs/providers/xinference) |  |  |  |  | ✅ |  |
| [FriendliAI](https://docs.litellm.ai/docs/providers/friendliai) | ✅ | ✅ | ✅ | ✅ |  |  |
| [Galadriel](https://docs.litellm.ai/docs/providers/galadriel) | ✅ | ✅ | ✅ | ✅ |  |  |

[**Read the Docs**](https://docs.litellm.ai/docs/)

## Contributing

[Permalink: Contributing](https://github.com/BerriAI/litellm#contributing)

Interested in contributing? Contributions to LiteLLM Python SDK, Proxy Server, and contributing LLM integrations are both accepted and highly encouraged! [See our Contribution Guide for more details](https://docs.litellm.ai/docs/extras/contributing_code)

# Enterprise

[Permalink: Enterprise](https://github.com/BerriAI/litellm#enterprise)

For companies that need better security, user management and professional support

[Talk to founders](https://calendly.com/d/4mp-gd3-k5k/litellm-1-1-onboarding-chat)

This covers:

- ✅ **Features under the [LiteLLM Commercial License](https://docs.litellm.ai/docs/proxy/enterprise):**
- ✅ **Feature Prioritization**
- ✅ **Custom Integrations**
- ✅ **Professional Support - Dedicated discord + slack**
- ✅ **Custom SLAs**
- ✅ **Secure access with Single Sign-On**

# Code Quality / Linting

[Permalink: Code Quality / Linting](https://github.com/BerriAI/litellm#code-quality--linting)

LiteLLM follows the [Google Python Style Guide](https://google.github.io/styleguide/pyguide.html).

We run:

- Ruff for [formatting and linting checks](https://github.com/BerriAI/litellm/blob/e19bb55e3b4c6a858b6e364302ebbf6633a51de5/.circleci/config.yml#L320)
- Mypy + Pyright for typing [1](https://github.com/BerriAI/litellm/blob/e19bb55e3b4c6a858b6e364302ebbf6633a51de5/.circleci/config.yml#L90), [2](https://github.com/BerriAI/litellm/blob/e19bb55e3b4c6a858b6e364302ebbf6633a51de5/.pre-commit-config.yaml#L4)
- Black for [formatting](https://github.com/BerriAI/litellm/blob/e19bb55e3b4c6a858b6e364302ebbf6633a51de5/.circleci/config.yml#L79)
- isort for [import sorting](https://github.com/BerriAI/litellm/blob/e19bb55e3b4c6a858b6e364302ebbf6633a51de5/.pre-commit-config.yaml#L10)

If you have suggestions on how to improve the code quality feel free to open an issue or a PR.

# Support / talk with founders

[Permalink: Support / talk with founders](https://github.com/BerriAI/litellm#support--talk-with-founders)

- [Schedule Demo 👋](https://calendly.com/d/4mp-gd3-k5k/berriai-1-1-onboarding-litellm-hosted-version)
- [Community Discord 💭](https://discord.gg/wuPM9dRgDw)
- Our numbers 📞 +1 (770) 8783-106 / ‭+1 (412) 618-6238‬
- Our emails ✉️ [ishaan@berri.ai](mailto:ishaan@berri.ai) / [krrish@berri.ai](mailto:krrish@berri.ai)

# Why did we build this

[Permalink: Why did we build this](https://github.com/BerriAI/litellm#why-did-we-build-this)

- **Need for simplicity**: Our code started to get extremely complicated managing & translating calls between Azure, OpenAI and Cohere.

# Contributors

[Permalink: Contributors](https://github.com/BerriAI/litellm#contributors)

[![](https://camo.githubusercontent.com/8e29b23dcec9d07b46521758c401a2f3e4906ffe41e35179bd9908e7c4eeaa2a/68747470733a2f2f636f6e747269622e726f636b732f696d6167653f7265706f3d426572726941492f6c6974656c6c6d)](https://github.com/BerriAI/litellm/graphs/contributors)

## Run in Developer mode

[Permalink: Run in Developer mode](https://github.com/BerriAI/litellm#run-in-developer-mode)

### Services

[Permalink: Services](https://github.com/BerriAI/litellm#services)

1. Setup .env file in root
2. Run dependant services `docker-compose up db prometheus`

### Backend

[Permalink: Backend](https://github.com/BerriAI/litellm#backend)

1. (In root) create virtual environment `python -m venv .venv`
2. Activate virtual environment `source .venv/bin/activate`
3. Install dependencies `pip install -e ".[all]"`
4. Start proxy backend `uvicorn litellm.proxy.proxy_server:app --host localhost --port 4000 --reload`

### Frontend

[Permalink: Frontend](https://github.com/BerriAI/litellm#frontend)

1. Navigate to `ui/litellm-dashboard`
2. Install dependencies `npm install`
3. Run `npm run dev` to start the dashboard

## About

Python SDK, Proxy Server (LLM Gateway) to call 100+ LLM APIs in OpenAI format - \[Bedrock, Azure, OpenAI, VertexAI, Cohere, Anthropic, Sagemaker, HuggingFace, Replicate, Groq\]


[docs.litellm.ai/docs/](https://docs.litellm.ai/docs/ "https://docs.litellm.ai/docs/")

### Topics

[gateway](https://github.com/topics/gateway "Topic: gateway") [bedrock](https://github.com/topics/bedrock "Topic: bedrock") [openai](https://github.com/topics/openai "Topic: openai") [vertex-ai](https://github.com/topics/vertex-ai "Topic: vertex-ai") [azure-openai](https://github.com/topics/azure-openai "Topic: azure-openai") [llm](https://github.com/topics/llm "Topic: llm") [langchain](https://github.com/topics/langchain "Topic: langchain") [llmops](https://github.com/topics/llmops "Topic: llmops") [anthropic](https://github.com/topics/anthropic "Topic: anthropic") [openai-proxy](https://github.com/topics/openai-proxy "Topic: openai-proxy") [ai-gateway](https://github.com/topics/ai-gateway "Topic: ai-gateway") [llm-gateway](https://github.com/topics/llm-gateway "Topic: llm-gateway")

### Resources

[Readme](https://github.com/BerriAI/litellm#readme-ov-file)

### License

[View license](https://github.com/BerriAI/litellm#License-1-ov-file)

### Security policy

[Security policy](https://github.com/BerriAI/litellm#security-ov-file)

[Activity](https://github.com/BerriAI/litellm/activity)

[Custom properties](https://github.com/BerriAI/litellm/custom-properties)

### Stars

[**20.2k**\\
stars](https://github.com/BerriAI/litellm/stargazers)

### Watchers

[**109**\\
watching](https://github.com/BerriAI/litellm/watchers)

### Forks

[**2.6k**\\
forks](https://github.com/BerriAI/litellm/forks)

[Report repository](https://github.com/contact/report-content?content_url=https%3A%2F%2Fgithub.com%2FBerriAI%2Flitellm&report=BerriAI+%28user%29)

## [Releases\  814](https://github.com/BerriAI/litellm/releases)

[v1.65.3-nightly.post1\\
Latest\\
\\
Apr 4, 2025](https://github.com/BerriAI/litellm/releases/tag/v1.65.3-nightly.post1)

[\+ 813 releases](https://github.com/BerriAI/litellm/releases)

## Sponsor this project

- [https://buy.stripe.com/9AQ03Kd3P91o0Q8bIS](https://buy.stripe.com/9AQ03Kd3P91o0Q8bIS)

## [Packages\  5](https://github.com/orgs/BerriAI/packages?repo_name=litellm)

- [litellm](https://github.com/orgs/BerriAI/packages/container/package/litellm)
- [litellm-database](https://github.com/orgs/BerriAI/packages/container/package/litellm-database)
- [litellm-helm](https://github.com/orgs/BerriAI/packages/container/package/litellm-helm)

[\+ 2 packages](https://github.com/orgs/BerriAI/packages?repo_name=litellm)

## [Used by 9.2k](https://github.com/BerriAI/litellm/network/dependents)

[- ![@phuctvu](https://avatars.githubusercontent.com/u/72380793?s=64&v=4)\\
- ![@truefoundry](https://avatars.githubusercontent.com/u/93512441?s=64&v=4)\\
- ![@dylan-teehan-skc](https://avatars.githubusercontent.com/u/96421021?s=64&v=4)\\
- ![@cleancodify](https://avatars.githubusercontent.com/u/198091403?s=64&v=4)\\
- ![@Advaith2805](https://avatars.githubusercontent.com/u/131602142?s=64&v=4)\\
- ![@Luphonix-Prime](https://avatars.githubusercontent.com/u/202592061?s=64&v=4)\\
- ![@hazlamahedich](https://avatars.githubusercontent.com/u/65190465?s=64&v=4)\\
- ![@Chethiya99](https://avatars.githubusercontent.com/u/94435820?s=64&v=4)\\
\\
\+ 9,197](https://github.com/BerriAI/litellm/network/dependents)

## [Contributors\  496](https://github.com/BerriAI/litellm/graphs/contributors)

- [![@ishaan-jaff](https://avatars.githubusercontent.com/u/29436595?s=64&v=4)](https://github.com/ishaan-jaff)
- [![@krrishdholakia](https://avatars.githubusercontent.com/u/17561003?s=64&v=4)](https://github.com/krrishdholakia)
- [![@Manouchehri](https://avatars.githubusercontent.com/u/7232674?s=64&v=4)](https://github.com/Manouchehri)
- [![@msabramo](https://avatars.githubusercontent.com/u/305268?s=64&v=4)](https://github.com/msabramo)
- [![@dependabot[bot]](https://avatars.githubusercontent.com/in/29110?s=64&v=4)](https://github.com/apps/dependabot)
- [![@yujonglee](https://avatars.githubusercontent.com/u/61503739?s=64&v=4)](https://github.com/yujonglee)
- [![@vincelwt](https://avatars.githubusercontent.com/u/5092466?s=64&v=4)](https://github.com/vincelwt)
- [![@coconut49](https://avatars.githubusercontent.com/u/3363189?s=64&v=4)](https://github.com/coconut49)
- [![@simonsanvil](https://avatars.githubusercontent.com/u/37579399?s=64&v=4)](https://github.com/simonsanvil)
- [![@rick-github](https://avatars.githubusercontent.com/u/14946854?s=64&v=4)](https://github.com/rick-github)
- [![@ShaunMaher](https://avatars.githubusercontent.com/u/6510825?s=64&v=4)](https://github.com/ShaunMaher)
- [![@SunnyWan59](https://avatars.githubusercontent.com/u/94445569?s=64&v=4)](https://github.com/SunnyWan59)
- [![@paul-gauthier](https://avatars.githubusercontent.com/u/69695708?s=64&v=4)](https://github.com/paul-gauthier)
- [![@paneru-rajan](https://avatars.githubusercontent.com/u/4735661?s=64&v=4)](https://github.com/paneru-rajan)

[\+ 482 contributors](https://github.com/BerriAI/litellm/graphs/contributors)

## Languages

- [Python91.4%](https://github.com/BerriAI/litellm/search?l=python)
- [TypeScript7.7%](https://github.com/BerriAI/litellm/search?l=typescript)
- [HTML0.7%](https://github.com/BerriAI/litellm/search?l=html)
- [JavaScript0.2%](https://github.com/BerriAI/litellm/search?l=javascript)
- [Shell0.0%](https://github.com/BerriAI/litellm/search?l=shell)
- [Dockerfile0.0%](https://github.com/BerriAI/litellm/search?l=dockerfile)

You can’t perform that action at this time.